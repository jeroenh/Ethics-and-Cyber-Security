\documentclass{svjour3}                     % onecolumn (standard format)
%
% \smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{hyperref}
\journalname{Science and Engineering Ethics}
%
\begin{document}

\title{Ethics and Cyber Security}
% \subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Kevin Macnish         \and
        Jeroen van der Ham}

%\authorrunning{Short form of author list} % if too long for running head

\institute{F. Author \at
              first address \\
              Tel.: +123-45-678910\\
              Fax: +123-45-678910\\
              \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           S. Author \at
              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\title{Research Ethics in Cyber Security}
% \subtitle{An Overview of Ethical Research Needs}

\begin{abstract}
fooThis paper sets out a number of ethical issues facing the challenge of research in the cybersecurity community.  We separate these issues into those facing the academic research community and those facing the (corporate) practitioner community.  While there is overlap between these communities, there are also stark differences.  In particular, academic researchers can often rely on research ethics committees (or Institutional Research Boards, IRBs) to provide ethical oversight and governance which are typically unavailable to the practitioner community.  However, we argue that even within the academic community the constitution of IRBs is such that they may be (and in some cases at least are) unable to offer sound advice. Our conclusion is that codes of conduct should be developed and deployed provided they can be seen to be effective. In tandem with this, an active discussion regarding the ethics of cybersecurity is urgently needed.

Keywords: cybersecurity, ethics, privacy, IRB, personal data, discrimination, trust, research

\end{abstract}

\section{Introduction}
\label{intro}
In this paper we argue that current methods of ethical oversight regarding cyber security ethics are inadequate. These methods fail in two areas: university-based development and in the broader community of practising cyber security experts. In the former the problems stem from a lack of awareness among members of ethical review committees as to the nature of the ethical problems regarding cyber security. In the latter the problems are widely known, but a lack of adequate guidance or accountability forms a barrier to consistent ethical practice. We are not claiming that current cyber security development or practice are unethical. Rather, our point is that these practices go largely ungoverned and unguided, despite the clear potential for significant harm. We argue that there hence needs to be a greater appreciation of the risks of cyber security development in ethical review committees and clear codes of conduct for the professional community which cover both development and practice.

The paper opens with a case study regarding academic research into cybersecurity which was ethically flawed, but which genuinely sought ethics committee approval. This approval was denied, not because of the flaws in the case but rather because the case did not raise obvious issues of personally identifiable information. This suggests that the ethics committees in the institutions consulted had a worryingly narrow view of ethical issues in their own field of research. We then list ethical issues which include, but go much further than, privacy and the confidential handling of personally identifiable information.

In the second part of the paper we move to look at a case study concerning research in the non-academic practitioner sector. Here again we note ethical flaws in the research which, in this case, arguably went unnoticed due to the absence of adequate ethical oversight. As with Part I, we follow the case study with a list of perceived ethical issues pertaining to practitioner research in cybersecurity. Some, but not all, of these issues overlap with those faced by the academic community. We conclude with a call for a mature discussion on ethical issues in the realms of cybersecurity research which embraces but also goes beyond concerns with privacy.


\section{Part I}
\subsection{Case Study 1}
\label{sec:case1}

\begin{quote}Statement from the SIGCOMM 2015 Program Committee: The SIGCOMM 2015 PC appreciated the technical contributions made in this paper but found the paper controversial because some of the experiments the authors conducted raise ethical concerns. The controversy arose in large part because the networking research community does not yet have widely accepted guidelines or rules for the ethics of experiments that measure online censorship. In accordance with the published submission guidelines for SIGCOMM 2015, had the authors not engaged with their Institutional Review Boards (IRBs) or had their IRBs determined that their research was unethical, the PC would have rejected the paper without review. But the authors did engage with their IRBs, which did not flag the research as unethical. The PC hopes that discussion of the ethical concerns these experiments raise will advance the development of ethical guidelines in this area. It is the PC’s view that future guidelines should include as a core principle that researchers should not engage in experiments that subject users to an appreciable risk of substantial harm absent informed consent. The PC endorses neither the use of the experimental techniques this paper describes nor the experiments the authors conducted
	\end{quote} \cite{burnett2015encore}.


The above warning was placed at the header of an article accepted by the SIGCOMM Program Committee in 2015.  The paper in question concerned creating scripts to monitor levels of censorship.  The scripts (called Encore) were then placed on the webservers of obliging companies (or on dummy advertising sites) and seamlessly transferred to the computers of clients when they visited the particular servers.  From clients' computers, Encore would then try to access sites that were likely to be censored and sent information about their success or lack thereof back to the designers of the script.  At no point were clients aware that Encore was running on their computer, still less were they asked for consent to have it operating from their computer.  

At the time of writing, at least 17 companies had deployed Encore on their webservers, which led to ``141,626 measurements from 88,260 distinct IPs in 170 countries, with China, India, the United Kingdom, and Brazil reporting at least 1,000 measurements, and more than 100 measurements from Egypt, South Korea, Iran, Pakistan, Turkey, and Saudi Arabia. These countries practice some form of Web filtering" \cite[p.~662]{burnett2015encore}

This last sentence is something of an understatement.  In many of these countries, the mere act of visiting a banned site may lead to further investigations by the security services and may highlight individuals as persons of potential interest.  This would be bad enough for any unwitting user, but if the client happens to be a dissident writing for free speech in his or her country, then the act of running the script from their computer could alert the security services to their activities. In the words of one of the SIGCOMM Program Committee members, the requests ``could potentially result in severe harm: for example, when the user lives in a regime where due process for those seen as requesting censored content may not exist" \cite{byers2015encore}.

As noted in the opening comment, the authors did recognize (some of) the ethical concerns with their work.  They determined only to have the scripts attempt to connect with sites that were not overly contentious, such as Facebook, YouTube and Twitter, and explicitly recognized that, ``deploying a tool like Encore engenders risks that we need to better understand" \cite[p.~663]{burnett2015encore}. They go on to say that, ``balancing the benefit and risk of measuring filtering with Encore is difficult. This paper has made the benefit clear" but that, ``striking this balance between benefit and risk raises ethical questions that researchers in computer science rarely face and that conventional ethical standards do not address. As such, our understanding of the ethical implications of collecting measurements using Encore has evolved, and the set of measurements we collect and report on has likewise changed to reflect our understanding" \cite[p.~663]{burnett2015encore}.

The paper proceeds to list attempts by the authors to have the measurement collection reviewed by Institutional Review Boards (IRBs) at two leading US universities \cite[Table 2,~p.~662]{burnett2015encore}. Somewhat surprisingly, given the in principle plausibility of gaining informed consent from clients, both IRBs declined to formally review the proposal as it did not ``collect or analyse Personally Identifiable Information (PII) and [was] not human subjects research" \cite[p.~664]{burnett2015encore}. This, as we say, is surprising as human subject research is not merely that which collects or analyses PII, as the doctors at the Nuremburg Trials would have been ready to point out.

In discussing the decision not to request informed consent, the authors list several reasons.  These include the fact that ``there are classes of experiments that can still be conducted ethically without [informed consent], such as when obtaining consent is either prohibitive or impractical and there is little appreciable risk of harm to the subject" \cite[p.~664]{burnett2015encore}. While this is true, this does not appear to be a case where getting informed consent would be prohibitive and, even if it were, there is appreciable risk to participants and so the research would not class as low-risk observation.  Indeed, as Byers notes, ``PC members and survey respondents of an independent study agreed that most users for whom censorship is an issue would be unlikely to consent to Encore's measurements" \cite{byers2015encore}. A second reason given for deciding not to request informed consent was that doing so ``would require apprising a user about nuanced technical concepts, such as cross-origin requests and how Web trackers work —- and doing so across language barriers, no less. Such burdens would dramatically reduce the scale and scope of measurements, relegating us to the already extremely dangerous status quo of activists and researchers who put themselves into harm’s way to study censorship" \cite[p.~664]{burnett2015encore}. The desire to move beyond research that puts the researcher in harm’s way to study censorship is of course well-motivated.  However, informing a research subject about the potential harms of complex research, and doing so across language barriers, is standard practice for many researchers in the medical and social science fields.

The authors note further that, ``informed consent does not ever decrease risk to users; it only alleviates researchers from some responsibility for that risk and may even increase risk to users by removing any traces of plausible deniability" \cite[p.~664]{burnett2015encore}. This is cynical in the extreme and reads (to us) as post-hoc justification.  It is true that informed consent does not decrease risk to research participants, but the point is that participants should be given the opportunity to decide for themselves whether they wish to take those risks, and not have those risks imposed by researchers. t is the duty of the researcher to describe the risks to the participants in such a way that the participants can make an adequate decision in accepting those risks. Indeed, this does ``alleviates researchers from some responsibility'', but does so in a controlled context.  Consent is a central aspect of post-war research ethics and has underpinned the Nuremberg Trials, the Helsinki Declaration, and virtually all subsequent writings on research ethics.  In its place, the authors write that, ``we believe researchers should instead focus on reducing risk to uninformed users, as we have done with repeated iteration after consultation with ethics experts. It is generally accepted that users already have little control over or knowledge of much of the traffic that their Web browsers and devices generate (a point raised by Princeton’s office of research integrity and assurance), which already gives users reasonable cover. By analogy, the prevalence of malware and third-party trackers itself lends credibility to the argument that a user cannot reasonably control the traffic that their devices send'' \cite[p.~664-65]{burnett2015encore}. We agree that researchers should focus on reducing risk to participants, but this should not come as a zero-sum game with informed consent.  Both should be present. Finally, the point raised by Princeton's office of research integrity and assurance is worrying, not least because of its source.  It may be that in a country in which censorship is rarely practiced such a defence might be plausible, but history has shown that security services in totalitarian states tend to be extremely sensitive to such activities and often prefer an overly cautious perspective that ends in people being incarcerated who should not be.

Our arguments may seem harsh here, especially as the authors worked to discuss ``Encore with ethics experts at the Oxford Internet Institute, the Berkman Center, and Citizen Lab, and our follow-on work examines broader ethical concerns of censorship measurement. We have also been working with the organizers of the SIGCOMM NS Ethics workshop, which we helped solicit, to ensure that its attendees will gain experience applying principled ethical frameworks to networking and systems research, a process we hope will result in more informed and grounded discussions of ethics in our community" \cite[p.~664]{burnett2015encore}. We applaud the efforts of the authors and share their hopes in more informed and grounded discussions of ethics in the community, and it is to this latter end that we focus this paper.

We do not write this to condemn the authors or the IRBs that allowed these experiments to proceed. However, the forgoing case study amply demonstrates the paucity of ethical awareness within the academic computer science community at both researcher and IRB level. While there are notable exceptions, we believe that this generally comes from a lack of ethics tuition within computer science and a lack of computer science at IRB level.


\section{Cyber security Development in Academic Contexts}
\label{sec:academicdev}
As noted in the introduction, developments in cyber security methodology, tactics and techniques occur at both the level of academic research and in research at a corporate and government (i.e. practitioner) level.  This is not to say that academic institutions do not practice cyber security: they do. However, at the stage of practice, the academic institution becomes indistinguishable from the corporate or government practice of cyber security. While many of these ethical issues will invariably overlap, each of these also raises its own concerns. We present a summary of those issues experienced in our own (academic) work below. In Part II we list ethical issues pertaining to practitioner-led research.

\subsubsection{Informed consent}
The ability to and act of gaining informed consent from those who are affected by the research.  This is seen starkly in the above case of testing censorship systems. It may also be a factor when the system is neither owned nor operated by the researcher. In such cases, should permission be required for the system to be tested?

The justification for informed consent is disputed as to whether it is rooted in the autonomy of the research subject \cite{beauchamp2009autonomy} or in the principle of minimizing harm to the research subject \cite{manson2007rethinking}. However, whichever approach is correct, the seeking and gaining of informed consent has been the backbone of research ethics in which people may be harmed in the post-war period and cannot be lightly ignored.

\subsubsection{Protection of subjects from inadvertent harm}
It is wholly plausible that there are cybersecurity research projects in which people may stand to suffer as a result of that research, again as illustrated in the introductory case above. We take it as given that harm is not intended on research subjects, but an absence of intention does not amount to an absence of effect: unintended harm is harm nonetheless. However, there may be some confusion here as to standard ethical practice. In arguing that cybersecurity research ethics should draw from clinical research ethics, Tyler Moore and Richard Clayton, for instance, argue that in the event of recognizing harm arising, researchers should only stop the trial when the results are ``statistically significant \emph{and} the divergence in treatment outcome is substantial [italics added]'' \cite[p.~15]{moore2011ethical}. However, this is not standard practice. There is an additional principle of minimization of harm to the participant which overrides the interest of the research, particularly in cases where little or no consent has been given.

\subsubsection{Privacy}
Conducting research will often reveal personal data/personally identifiable information (PII), which then needs to be handled appropriately. Legal sensitivity to privacy concerns is markedly varied, with the European General Data Protection Regulation imposing strong restrictions and punitive measures to any engaging with data pertaining to European citizens, while the data of individuals in the Americas, Asia or Africa is far less subject to regulation. This could lead to ``data dumping'' in which research is carried out in countries with lower barriers for use of personal data rather than jump through bureaucratic hurdles in Europe.  The result is that the data of non-European citizens is placed at higher risk than that of Europeans.

\subsubsection{Reporting of incidental findings}
In the course of discovering personal data/PII, further information relating to an individual or organisation may be discovered about which that person or organisation may or may not know.  Decisions need to be made in advance as to whether and how to inform that entity if appropriate. For example, evidence may emerge that a member of an organisation is seeking employment elsewhere, or that the spouse of an employee is having an affair with another employee. In the absence of a policy written in advance, such discoveries become ethical dilemmas in a way which they need not be.

\subsubsection{Coordinated vulnerability disclosure}
Where vulnerabilities are discovered, should these be disclosed to a pertinent authority? Such an authority may be a company using the software which has the vulnerability, a third-party provider of that software, or a state entity which oversees vulnerabilities. In principle, a broad awareness of vulnerabilities is a positive as it can help the community come together to get a clear picture of how widespread the vulnerability is, whether any proprietary patches have been developed, and whether the vulnerability has been exploited. However, there is also the risk in broadcasting the vulnerability, even within a small community of cybersecurity professionals, that knowledge of that vulnerability will leak and could thereby be exploited.

\subsubsection{Potential for prosecution if vulnerabilities are shared}
Following on from the problems of vulnerability disclosure, there have been cases (such as the research we discuss below in case study 2) where vulnerabilities are discovered but no informed consent was obtainable. In such cases there may be a risk that the effected party may prosecute the university or researcher. In such cases, is there a duty to make those discoveries known? What degree of risk should the researcher and the research institution each burden in investigating such vulnerabilities?

\subsubsection{Testing on live and sensitive systems}
Some systems cannot be taken off line in order to carry out research on them. This may be because they fulfil a vital function related to critical national infrastructure or because there is no built-in redundancy to the system. In such cases, there is a risk of carrying out research that may have an impact on the functioning of that system. At the same time, such systems need to be tested for security purposes, possibly more so than their commercial counterparts. The preferable solution here would be for the existence of redundancy in the system such that it can be tested a part at a time without risk to the whole, but this is clearly not always feasible. When this redundancy is not present and there is a risk of damaging the system, how far should the researcher go in testing that system?

\subsubsection{Impact on commercial viability of a system}
If vulnerabilities are found and not patched immediately, this could have an impact on the commercial viability of the system.  Does the researcher have a (whistleblowing) duty to make such unpatched vulnerabilities public in order that greater pressure is put on the owner of the system to resolve the fault? Again, this is illustrated in the illustration below of our case study 2, but it is a problem which is faced by universities as well as commercial testers.

\subsubsection{Hiding code in firmware using leaked vulnerabilities}
Drawing back to the Encore case above, should leaked vulnerabilities be used to install code on systems? In some cases, the researcher must act like a hacker in order to fully test the system, and this will include using vulnerabilities. How far is it reasonable for the researcher to go in carrying this out, though?

\subsubsection{Recognizing ethical problems}
While the authors of the Encore research did recognize and attempt to seek assistance from more than one IRB, this is not always the case. In many instances researchers do not even recognize the potential for an ethical issue to arise. In discussing ethically-questionable research on the Tor network carried out in 2008, Christopher Soghoian notes that the researchers ``simply did not see the ethical or legal issues associated with their data gathering'' \cite[p.~146]{soghoian2011enforced}, although he goes on to quote one of the researchers as saying that they had been ``advised that [seeing IRB guidance or approval] wasn’t necessary,'' suggesting that they had at least started to investigate the possibility of ethical issues arising \cite[p.~147]{soghoian2011enforced}. Following the presentation of the research in 2008, the University of Colorado announced that the researchers had not violated university ethics policies as ``by any reasonable standard, the work in question was not classifiable as human subject research'' (quoted in \cite{soghoian2011enforced}).

\subsubsection{Competence of IRBs}
Ultimately, one of the key concerns of this paper is that IRBs tend to consist of experts in ethics rather than experts in computer science, or vice versa.  In our experience, which is echoed in the above case study, it is difficult to find an IRB which combines both sets of expertise. Nor are we the first to point this out, a similar point has been made repeatedly over the last decade: in 2008 by Simson Garfinkel \cite{garfinkel2008irbs} and Mark Allman \cite{allman2008ought}, in 2010 by Carl Landwehr \cite{landwehr2010drawing} and in 2011 by \cite{johnson2011computer} and \cite[p.138-39]{aycock2011human}. Despite being raised a number of times, the problem persists as many computer science researchers have received only elementary education in ethics, some of which might have included research ethics, while many ethicists have very little understanding of computer science research methods. This is not to say that there is no cross-over as there clearly is, but this is not as wide-spread as it needs to be in order to effectively oversee the developments with which we are concerned in this paper.

The result is that, as noted in the introductory case study, IRBs in computer science tend to react to well-recognized ethical and legal problems such as personally identifiable information. Less concern is directed towards the potential harm that may arise to individual research participants, particularly when they have not given or are unable to give informed consent to participate in the research. Standardly this is only permissible in cases of observational research in which the risk of harm is deemed (by an independent IRB) to be low. Cybersecurity research is often more interactive than mere observational research and the potential for harm may be considerable.

\subsection{Summary}
\label{sec:summary}
In summary, there are a number of ethical issues regarding cybersecurity research, many of which we have highlighted here.  While these are obviously of ethical concern, we feel that in many cases, such as that highlighted in Case Study 1, most university IRBs are simply not up to the requirements of offering effective oversight and guidance to researchers. This is primarily owing to the lack of joint expertise in computer science and ethics, which in turn stems from a weak commitment of computer science and philosophy departments at undergraduate level to teach ethics to computer scientists.

\section{Part II - Cyber security Development in Industry Contexts }
\label{sec:practice}
\subsection{Case Study 2}
\label{sec:case2}
In August 2016, independent security research group MedSec purchased and attempted to attack a number of St. Jude Medical devices, including pacemakers and heart monitoring devices designed for home use. The team claimed to find multiple vulnerabilities in the home monitoring devices, including those which could be used to influence the behaviour of the pacemakers.

Rather than disclosing this to St. Jude Medical directly, MedSec teamed up with the investment firm Muddy Waters to short the stock of St. Jude Medical. They then released partial information about the vulnerabilities to the public, again without having informed St Jude Medical about the problems.  In the event, the stock dipped marginally but not such that MedSec made significant profits from the venture.

Initially St. Jude denied the claims regarding vulnerabilities and argued that their software was secure. This appeared to be supported by researchers at the University of Michigan, who claimed to be unable to reproduce the same malfunctions found by MedSec. The same day, Muddy Waters released a video purportedly demonstrating some vulnerabilities, which may have been created using some bad assumptions about how this device should be configured or used \cite{spring16medsec}. St Jude Medical responded by bringing a law suit against MedSec in September 2016 \cite{weigelt16jude,nichols16jude}.

Independent research by Bishop Fox, published in October 2016, supported the claims of MedSec, agreeing that there were some vulnerabilities in the St. Jude systems \cite{livitt2016prelim}. In August 2017 the US Federal Drug Administration subsequently recalled 465,000 pacemakers manufactured by Abbott Laboratories, who acquired St Jude Medical in January that year \cite{spring16medsec}.

Understandably, MedSec were criticised for working with Muddy Waters for, ironically, muddying the waters around their case. The independence of MedSec’s research was brought into question if they could receive financial reward for their findings through harming St Jude Medical on the stock market via reputation damage. MedSec’s CEO Justine Bone responded that the company had deliberated over which course to take and concluded that the collaboration with Muddy Waters was the best option to force St Jude Medical into taking action.  She claimed that St Jude had a poor history of responding to security flaws and referenced a reported case in which the company took two years to respond to a security flaw learning of its existence.  This history, she concluded, led MedSec to the conclusion that ``a partnership with Muddy Waters was the fastest route to improved product safety, improving patient safety and a better understanding of the risks faced by patients'' \cite{weigelt16jude}. 

On the one hand it seems as if MedSec were pre-judging St Jude Medical’s likely response to the revelation of the security flaws. The grounds that St Jude Medical would not respond in a timely fashion appear weak and based on generalised industry behaviour and a rumour of foot-dragging in response to prior revelations. Furthermore, MedSec’s motivations were brought into question by their decision to work with Muddy Waters to profit from shorting the stock as the flaws were announced. As David Robinson and Alex Halderman note, ``researchers must be vigilant to retain as much independence as is feasible – and transparent about the extent to which their end product is informed or shaped by other actors'' \cite[p.~122]{robinson2011ethical}.

On the other hand, MedSec’s concerns regarding industry foot-dragging were not misplaced. The traditional course of events, for independent security researchers to by-pass customers and inform vendors of flaws in their systems, has led to delays in patches being developed and legal cases under the Digital Millennium Copyright Act and the Computer Fraud and Abuse Act being brought against researchers. Furthermore, the independent research by Bishop Fox confirms MedSec’s claims which were flatly denied by St Jude Medical \cite{weigelt16jude}. Finally, VP of research at Veracode, Chris Eng has suggested that the MedSec precedent ``has a lot of potential to be a net positive. We’ve all seen how consumer products are often designed and built in insecure ways, and let’s face it, there has been virtually no improvement unless there’s a major financial or reputational impact in doing so'' \cite{spring16medsec}.

Our interest in the MedSec/Muddy Waters/St Jude Medical case here is two-fold. In the first instance, should MedSec have partnered with an investment company to short the stock of a company it knew would suffer on the market once the flaws were made known? Secondly, should cybersecurity researchers be protected from legal action such as the attempt to sue MedSec pursued by St Jude? In both cases, the answers are unclear. In an ideal world perhaps, uncalled-for penetration testing would be ``pure'' of financial motives, but we do not live in an ideal world and pen testers, especially those who appear motivated to work in the public interest, need to be recompensed somehow. Likewise, researchers with genuine, public-spirited motivations should be protected from predatory practices by companies seeking to paper over cracks in their own security through legal action. However, current conventions as to how to proceed with disclosure of vulnerabilities seem to be skewed in the favour of corporations and against the interests of the public. 

As is frequently the case with ethical dilemmas, there are no easy solutions to these issues. We argue below that the best way forward here is the development of a code of conduct for cybersecurity research which establishes what is and is not acceptable behaviour under these circumstances. Such a code will not only serve to guide cybersecurity researchers who do not have the privilege of a university umbrella shielding them from legal action, but it will also provide support to those researchers who act within the guidelines and can thereby reasonably claim to be operating within the recognized ethical boundaries of the field of practice. We highlight below several ethical issues raised in security practice before turning to the issue of codes of conduct.


\subsection{Summary of Issues raised in Cybersecurity Practice}
\label{sec:industryissues}
As noted in the second case study, outside the university environment further ethical problems arise for those engaged in cybersecurity practice.  These include many of the same issues faced by university research but are frequently complicated by a lack of institutional tradition and policy governing behaviour and conflicts of interest between making money and doing ``the right thing''. The fact remains that a university exists for the development of knowledge aimed at furthering the public good.  While many companies might see themselves fulfilling a similar role, for many others the concern is not endangering the public good rather than seeking to further it.

We break down the ethical issues related to cybersecurity practice into four broad areas: privacy and control-related issues, security issues, fairness issues and business interests. As will be apparent, these broad area titles are somewhat arbitrary, though, and many issues could be placed into several of these areas.

\subsubsection{Privacy- and Control-related Issues}
As with university research, privacy and control of data are key issues in cybersecurity. Practitioners are likely to encounter personal data on a regular basis, whether they are interested in this or not, and much of it will be of a sensitive nature, pertaining to bank or health records, for example.  The maintenance of privacy is thus crucial and professional standards of confidentiality must be maintained. However, privacy is not the only issue at stake with personal data. There is also a central concern regarding the control of those data. For example, the recent scandal concerning Facebook and Cambridge Analytica was not primarily a matter of privacy, but of what was done with people’s data, and the sharing of those data without consent \cite{ienca2018cambridge}). While the Cambridge Analytica scandal was not per se a matter of cybersecurity, it highlights such issues which concern the control of data but not the privacy of data. Harms then emerge which go beyond the revelation of otherwise private information to the potential misuse of data (e.g. to influence election results) and may, if the data are handled ineptly, have a detrimental effect on the quality, integrity and future usability of those data.

A related concern is access to personal and/or sensitive data which may come accidentally through researching potential vulnerabilities in a related system. For example, if two networks are connected, then exploring a vulnerability in one may lead the researcher into the connected network (potentially unowned by the company employing the researcher) and through that to personal data \cite[p.~124]{robinson2011ethical}. This would involve a clear privacy infraction, although not necessarily a violation on the part of the researcher if no intrusion into private data was intended.

\subsubsection{Security-related Issues}
There are obvious security-related issues at the heart of cybersecurity practice and if security is seen as an ethical issue, as we see it, the maintenance of adequate security for those data is itself an ethical issue. As such, compromises of security through insufficient funding, poor oversight of systems, late or no installation of patches, how and where data is stored, how that data is accessed, and poor training of staff in security awareness are all ethical concerns. Many of these issues may amount to professional negligence (see, for example, the microsite server discovered at Greenwich University containing databases of nearly 20,000 people, including staff and students, which contained references to sensitive issues such as mental health which had not been updated or apparently even managed for 12 years\footnote{\url{https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2018/05/the-university-of-greenwich-fined-120-000-by-information-commissioner-for-serious-security-breach/} , \url{https://nakedsecurity.sophos.com/2018/05/22/server-what-server-site-forgotten-for-12-years-attracts-hacks-fines/}}. At the same time, security is everyone’s concern and so responsibility for security should not be seen to rest solely on the heads of those charged with oversight of cybersecurity. If, after training, a CEO opens an attachment in a spear-phishing email, there is only so much a cybersecurity officer can do. Furthermore, limits in security budgets demand priorities are made, and as such some areas are likely to be under-resourced.

At the same time, it is clear that the risks of cyberattack frequently fail to be understood, which may be the cause of many of the above problems. A lack of funding, poor training of staff, a failure to install patches, etc. can all be hampered by a failure to recognize the immediacy or the gravity of the threat faced. One seemingly-obvious solution to this is to increase resources devoted to training, but this is not a panacea. We are not, for instance, likely to be able to train our way out of the problem of spear fishing CEOs.

An alternative solution is to raise the profile of cybersecurity professionals within an organisation such that they are seen as a benefit rather than a burden. Hence if a marketing department decides to launch a new website to support an ad campaign they could either rely on their own experience and training in cybersecurity or involve the cybersecurity team from the beginning of the project. Our preference would obviously be for the latter. Rather as ethics should be a key consideration for engineers from the start of a project rather than a tacked-on afterthought, there is only so much a cybersecurity officer will be able to do if invited to comment on a project the day before launch.

It is sometimes said, tongue in cheek, that there are two types of company: those that have been hacked and those that don’t know they have been hacked. The challenge posed by hacking raises a host of further ethical issues. What are the motivations for the hack and do motivations matter? Should there be a difference in response from a cybersecurity team if the hack is carried out by hacktivists rather than by criminals installing ransomware? Should cybersecurity ever ``hack back'' and try to follow the intruder to his or her source, and if so what happens then? Yet more problems arise when the hack is internal, coming from a disgruntled employee. To what degree is cybersecurity entitled to monitor the activity of all employees to identify and isolate those who may pose a risk? Is behavioural profiling of employees justified to automate the identification of suspicious activity? 

Also falling within this category are questions about disclosure. Key issues here include, who should be told in the event of a hack occurring: what mitigation strategies are in place and are these ethical? At what stage should clients, the police or the security services become involved? Should attribution be volunteered and if so by whom, bearing in mind that it may well serve the company and the government alike to attribute the attack to another state? As noted in the MedSec case, there are conventions regarding disclosure of discovered vulnerabilities and yet at least some corporations take advantage of these conventions. If there is a standard delay between disclosing to the company and disclosing to the public, then the company my drag its heals in finding a solution to the vulnerability. Furthermore, disclosure of the vulnerability to the public is damaging to the company or other companies using the same software (if a fix hasn’t been found) and may increase awareness of the vulnerability and hence aid future hacks. Robinson and Halderman note that there is a tension between those running vulnerable systems, for whom the appearance of a problem may be greater than the problem itself, and those using the systems, for whom the problem is of far greater concern than appearances that all is well \cite[p. 122--126]{robinson2011ethical}.

Related problems include whether security researchers should agree to so-called gagging clauses which prevent them from publicly disclosing vulnerabilities which the company in question then refuses to address. Again, Robinson and Halderman argue that ``researchers need to ensure that … rules allow them to maintain their independence. If researchers are asked to sign a nondisclosure agreement, they should ensure that the terms allow them to disclose problems they might find, and do not overly restrict their ability to perform future work'' \cite[p.~123]{robinson2011ethical}. In the same chapter, the authors consider challenges regarding the timing of disclosure of vulnerabilities in e-voting machines: too early and the researchers risk disrupting an election; too late and they risk damaging trust in the election result \cite[p.~126]{robinson2011ethical}.

As with accidental access to private data through connected networks discussed above, there are also possibilities to access machines in a similar manner. Such access can clearly lead to unintentional damage occurring to this equipment \cite[p.~125]{robinson2011ethical}.

Finally, the manner in which the company engaging in security research or audits responds to revelations either of flaws or a lack of discovered vulnerabilities is also a concern. We have already considered companies which do not attempt to address flaws and may impose gagging clauses on researchers (or worse, threaten legal action). However, there are also issues in companies taking negative results as demonstrations that their products are safe. It is clearly not the case that the failure of one researcher (or even many researchers) to find a flaw implies that there are no such flaws. Security researchers should be aware of how their findings will be interpreted and used.

\subsubsection{Fairness-related Issues}
The third area we identify is around fairness-related issues. While privacy/control and security focus on data, fairness issues focus more on the people behind those data. Hence a leading concern is again informed consent.  This remains a key issue for cybersecurity practice as it is for university research.  However, practitioners often lack a tradition or relevant policies regarding the sourcing of informed consent, and in many cases this may be effectively impossible to obtain if, for instance, the user base exceeds a few thousand individuals. Furthermore, as noted in the Facebook/Cornell University emotional response research of January 2014, there may be a tradition among some groups such as market research of not seeking informed consent \cite{meyer2014everything}. This then makes it difficult for the conscientious cybersecurity practitioner operating in these fields to insist on obtaining consent.

Trust is another area of concern, connecting the cybersecurity practitioner to those he or she is purportedly securing. Most people, we suspect, would reject a simplistic consequentialist calculus through which security trumps all other concerns and there is an increasing recognition that security is best practiced through relationship with those secured rather than imposed upon them. An antipathetic relationship here is in no-one’s interests, and yet security is often resented by employees and security teams often feel underappreciated. Responses to this might involve increased transparency and access to cybersecurity teams, a focus on developing diversity within those teams, and efforts made by those teams in engaging with the workforce. Even here questions remain, though, such as how far to push transparency: should it extend to government agencies or even other companies? On one hand sharing information increases vulnerability as one’s defences are known, and one’s experience of attacks shared, but on the other it is arguably only by pooling experience that an effective defence can be mounted \cite[p.89--111]{guiora2017cybersecurity}. While this is undeniably risky, similar decisions made to share vulnerabilities in physical security in the past have led to positive developments in inter-corporate relationships and safety \cite{turnbull15ethics}.

Diversity and related justice issues also extend beyond the composition of those engaged directly in cybersecurity to the impacts that cybersecurity efforts may have. For example, profiling behaviours outside cybersecurity practice has been demonstrated to embed bias in algorithmic code, and so similar attempts at profiling for the purposes of cybersecurity risk embedding similar discriminatory patterns \cite{macnish2012unblinking}. The composition of cybersecurity teams may help in the early identification of such patterns, as it might also in lowering risk thresholds which are typically higher for white men than for women or minorities.

In the above and throughout this paper we have made continual reference to risk without specifically highlighting this as an ethical issue, and yet it is. Key ethical issues involve who is deciding on as opposed to who is effected by risky decisions, what are acceptable risk thresholds, and how is risk calculated \cite{hansson2013ethics}. Empirical research suggests that white males tolerate higher levels of risk than women and non-white males, for instance, and that experts are similarly more risk tolerant than the general public \cite{hermansson2010towards}. These both argue for a greater level of diversity in the decision-making process and for greater levels of public engagement \cite[p.120--128]{robinson2011ethical}.

Finally, there is an ongoing problem with cybersecurity insofar as the locus of responsibility is concerned \cite[p.~70--88]{guiora2017cybersecurity}. This is less of a problem in academic research where the work is carried out under the auspices of an institution with its own IRBs and structural hierarchies. However, in commercial research the locus of responsibility, and how far that responsibility extends, is unclear to say the least. Should a company be entirely responsible for developing its own cybersecurity? Is this so even if the company is (likely to be) subject to attack from foreign states or state-backed hackers? To what degree should the state take responsibility for protecting its own economy on the internet as it does in physical space, by providing safe places to trade?

\subsubsection{Business Ethics}
The broad final area of ethical concern that we focus on is that of business ethics, or the conflicts that arise specifically because of competing interests in security and making money.  Clearly, one hopes, security will not be ignored entirely in the interests of channelling funds into obvious profit-making activities, and a minor degree of prescience will suggest that good security will strengthen a company’s reputation and client trust in that organisation.  Nonetheless, it would be naive to suggest that conflicts of interest do not emerge between individual interests, public interests and corporate interests. A matter of days before the 2017 Equifax breach was made public, certain senior executives sold their shares in the company. While they protest their innocence, they have been charged with insider trading. In the case of the MedSoft/St Jude breach discussed above, MedSoft were not acting on information that was only available to those inside the company and so the decision to short the stock was not a matter of insider trading. Nonetheless, the wisdom of such a move might be questioned, as might the decision to publicize the vulnerabilities rather than approach St Jude first, giving them time to develop patches. Finally, the decision of Marissa Meier, then CEO of Yahoo, not to inform the public of the hacks in 2013 and 2014 regarding 3bn accounts seems hard to understand in terms other than protection of the company’s image.

\subsection{Recommendations}
\label{sec:recommendations}
The aforementioned ethical issues are legion and complicated, albeit hardly new to the cybersecurity community.  Despite this, there is little guidance as to how practitioners should proceed in many of these cases.  One approach would be the development of codes of conduct, either at an institutional level, such as through the IEEE and ACM, or at the corporate level. While the IEEE and ACM each have developed codes of conduct, neither these nor the general principles spelled out in the Menlo principles address all the issues raised in this paper.  Furthermore, codes of conduct must be supported by effective sanctions if they are broken, otherwise they are worth little more than the paper they are printed on \cite{davis1991thinking}. We do not believe that such a support system exists in the field of cybersecurity and the field is worse off for it.

In addition to this, there is a clear need for the development of an active conversation regarding ethics in the research and practice of cybersecurity.  This, too, is lacking, owing in part to the relative paucity of ethics teaching provided to computer scientists in higher education.  However, through the publication of this paper we hope to stimulate further discussion at the academic and practitioner level regarding the ethical issues raised here, and doubtless others that we have overlooked.

\section{Conclusion}
\label{sec:conclusion}
In this paper we have argued that current methods of oversight and guidance regarding cybersecurity ethics are inadequate. We have considered these methods in two areas: university-based development and the community of practising experts. In the former we argued that the problems stem from a lack of awareness among members of ethical review committees as to the nature of relevant ethical problems, such as considered in Case Study 1. In the latter there is a lack of adequate guidance or accountability which forms a barrier to consistent ethical practice, illustrated in Case Study 2. We have therefore argued that there needs to be a greater appreciation of the risks of cyber security development in academic ethical review committees and clear (and enforceable) codes of conduct for, or at least active discourse within, the professional community which cover development and practice.

% \begin{acknowledgements}
% Acknowledgement to your first professor go here.
% %If you'd like to thank anyone, place your comments here
% %and remove the percent signs.
% \end{acknowledgements}

% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{bibliography}   % name your BibTeX data base

% Non-BibTeX users please use
% \begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
% \bibitem{RefJ}
% Format for Journal Reference
% Author, Article title, Journal, Volume, page numbers (year)
% Format for books
% \bibitem{RefB}
% Author, Book title, page numbers. Publisher, place (year)
% etc
% \end{thebibliography}



% \label Possibly cut: Ransomware Research
% Cyber security researchers have examined the behaviour of several different ransomware campaigns in \cite{huangtracking}. The study examined the initial vector of ransomware infections, which type of file were encrypted, and how payment was requested. Finally, these payments were tracked due to the public nature of the Bitcoin ledger.
% The authors of the paper have identified ethical dilemmas they faced when researching possible disruption options to ransomware campaigns. As an example, ransomware creators often put time limits on payments. Any disruption to the payment infrastructure may lead to victims being unable to access their files, or having their prices increased as ransom amounts often increase with time.

\end{document}

References
Allman, Mark. 2008. ``What Ought a Program Committee to Do?'' WOWCS 8: 1–5.
Aycock, John, Elizabeth Buchanan, Scott Dexter, and David Dittrich. 2012. ``Human Subjects, Agents, or Bots: Current Issues in Ethics and Computer Security Research.'' In Financial Cryptography and Data Security, edited by George Danezis, Sven Dietrich, and Kazue Sako, 138–45. Berlin: Springer.
Beauchamp, Tom L. 2009. ``Autonomy and Consent.'' In The Ethics of Consent: Theory and Practice, edited by Franklin Miller and Alan Wertheimer, 1 edition, 55–78. Oxford ; New York: OUP USA.
Davis, Michael. 1991. ``Thinking like an Engineer: The Place of a Code of Ethics in the Practice of a Profession.'' Philosophy & Public Affairs, 150–167.
Garfinkel, Simson L. 2008. ``IRBs and Security Research: Myths, Facts and Mission Creep.'' Proceedings of the 1st Conference on Usability, Psychology, and Security.
Guiora, Amos N. 2017. Cybersecurity: Geopolitics, Law, and Policy. 1 edition. Boca Raton, FL: Routledge.
Hansson, Sven Ove. 2013. The Ethics of Risk: Ethical Analysis in an Uncertain World. Palgrave Macmillan.
Hermansson, Hélène. 2010. ``Towards a Fair Procedure for Risk Management.'' Journal of Risk Research 13 (4): 501–15. https://doi.org/10.1080/13669870903305903.
Ienca, Marcello, and Effy Vayena. 2018. ``Cambridge Analytica and Online Manipulation.'' Scientific American Blog Network. March 30, 2018. https://blogs.scientificamerican.com/observations/cambridge-analytica-and-online-manipulation/.
Johnson, Maritza L., Steven M. Bellovin, and Angelos D. Kromytis. 2012. ``Computer Security Research with Huma Subjects: Risks, Benefits and Informed Consent.'' In Financial Cryptography and Data Security, edited by George Danezis, Sven Dietrich, and Kazue Sako, 131–37. Berlin: Springer.
Landwehr, Carl E. 2010. ``Drawing the Line.'' IEEE Security & Privacy 8 (1): 3–4.
Livitt, Carl D. 2016. ``Preliminary Expert Report of Carl D. Livitt.'' 0:16-cv-03002-DWF-JSM. https://medsec.com/stj_expert_witness_report.pdf.
Macnish, Kevin. 2012. ``Unblinking Eyes: The Ethics of Automating Surveillance.'' Ethics and Information Technology 14 (2): 151–67. https://doi.org/10.1007/s10676-012-9291-0.
Manson, Neil, and Onora O’Neill. 2007. Rethinking Informed Consent in Bioethics. Cambridge.
Meyer, Robinson. 2014. ``Everything We Know About Facebook’s Secret Mood Manipulation Experiment.'' The Atlantic. June 28, 2014. https://www.theatlantic.com/technology/archive/2014/06/everything-we-know-about-facebooks-secret-mood-manipulation-experiment/373648/.
Moore, Tyler, and Richard Clayton. 2012. ``Ethical Dilemmas in Take-Down Research.'' In Financial Cryptography and Data Security, edited by George Danezis, Sven Dietrich, and Kazue Sako, 146–53. Berlin: Springer.
Nichols, Shaun. 2016. ``St Jude Sues Short-Selling MedSec over Pacemaker ‘Hack’ Report.'' The Register. September 7, 2016. https://www.theregister.co.uk/2016/09/07/st_jude_sues_over_hacking_claim/.
Robinson, David G., and J. Alex Halderman. 2012. ``Ethical Issues in E-Voting Security Analysis.'' In Financial Cryptography and Data Security, edited by George Danezis, Sven Dietrich, and Kazue Sako. Lecture Notes in Computer Science 7126. Berlin: Springer.
Soghoian, Christopher. 2012. ``Enforced Community Standards for Research on Users of the Tor Anonymity Network.'' In Financial Cryptography and Data Security, edited by George Danezis, Sven Dietrich, and Kazue Sako, 146–53. Berlin: Springer.
Spring, Tom. 2016. ``Researchers: MedSec, Muddy Waters Set Bad Precedent With St. Jude Medical Short.'' The First Stop for Security News | Threatpost (blog). August 31, 2016. https://threatpost.com/researchers-medsec-muddy-waters-set-bad-precedent-with-st-jude-medical-short/120266/.
———. 2017. ``FDA Recalls 465K Pacemakers Tied to MedSec Research.'' The First Stop for Security News | Threatpost (blog). August 31, 2017. https://threatpost.com/fda-recalls-465k-pacemakers-tied-to-medsec-research/127750/.
Turnbull, John. 2015. ``Ethics and Employability.'' In Engineering in Society: Beyond the Technical, edited by Rob Lawlor, 2nd ed. Leeds: University of Leeds.
Weigelt, J.C. 2016. ``St. Jude Medical Brings Legal Action Against Muddy Waters and MedSec.'' September 7, 2016. http://media.sjm.com/newsroom/news-releases/news-releases-details/2016/St-Jude-Medical-Brings-Legal-Action-Against-Muddy-Waters-and-MedSec/default.aspx.



\begin{acknowledgements}
Acknowledgement to your first professor go here.
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
\end{acknowledgements}

% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{bibliography}   % name your BibTeX data base

% Non-BibTeX users please use
% \begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
% \bibitem{RefJ}
% Format for Journal Reference
% Author, Article title, Journal, Volume, page numbers (year)
% Format for books
% \bibitem{RefB}
% Author, Book title, page numbers. Publisher, place (year)
% etc
% \end{thebibliography}



\label Possibly cut: Ransomware Research

Cyber security researchers have examined the behaviour of several different ransomware campaigns in \cite{huangtracking}. The study examined the initial vector of ransomware infections, which type of file were encrypted, and how payment was requested. Finally, these payments were tracked due to the public nature of the Bitcoin ledger.

The authors of the paper have identified ethical dilemmas they faced when researching possible disruption options to ransomware campaigns. As an example, ransomware creators often put time limits on payments. Any disruption to the payment infrastructure may lead to victims being unable to access their files, or having their prices increased as ransom amounts often increase with time.

\end{document}
% end of file template.tex